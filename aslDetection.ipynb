{"cells":[{"cell_type":"code","source":["%pip install mediapipe"],"metadata":{"id":"yx5zALVdh1PW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MmSp4p1DdsS"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import cv2\n","import mediapipe\n","import os\n","import pandas as pd\n","import json\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from sklearn.preprocessing import OneHotEncoder\n","from nltk.corpus import brown\n","from nltk import FreqDist\n","import nltk\n","import keras\n","import keras.layers as layers\n","import einops\n","import mediapipe as mp"]},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"id":"aVw5ZIUqjLLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))"],"metadata":{"id":"QMcp3V_sa6y4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainDataset = keras.utils.image_dataset_from_directory(\n","  path+\"/asl_alphabet_train/asl_alphabet_train\",\n","  labels=\"inferred\",\n","  label_mode=\"int\",\n","  image_size=(224, 224),\n","  batch_size=1,\n","  shuffle=True,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123\n",")\n","\n","valDataset = keras.utils.image_dataset_from_directory(\n","  path+\"/asl_alphabet_train/asl_alphabet_train\",\n","  labels=\"inferred\",\n","  label_mode=\"int\",\n","  image_size=(224, 224),\n","  batch_size=1,\n","  shuffle=True,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123\n",")\n","\n","print(trainDataset.class_names)\n","mp_hands = mp.solutions.hands\n","\n","def cropHand(image):\n","    with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.001) as hands:\n","        results = hands.process(image)\n","        h, w, _ = image.shape\n","        if results.multi_hand_landmarks:\n","            x_min, y_min = w, h\n","            x_max, y_max = 0, 0\n","            for hand_landmarks in results.multi_hand_landmarks:\n","                for lm in hand_landmarks.landmark:\n","                    x, y = int(lm.x * w), int(lm.y * h)\n","                    x_min, y_min = min(x_min, x), min(y_min, y)\n","                    x_max, y_max = max(x_max, x), max(y_max, y)\n","            pad = 80\n","            x_min, y_min = max(x_min-pad, 0), max(y_min-pad, 0)\n","            x_max, y_max = min(x_max+pad, w), min(y_max+pad, h)\n","            cropped = image[y_min:y_max, x_min:x_max]\n","\n","            cropped_resized = cv2.resize(cropped, (224,224))\n","            return cropped_resized\n","        else:\n","            image *= 0\n","            return cv2.resize(image, (224,224))\n","\n","def preprocessWithMediapipe(x, y):\n","    img = x.numpy().astype(np.uint8)\n","\n","    if img.ndim == 2:\n","        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n","    elif img.shape[2] == 1:\n","        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n","    elif img.shape[2] == 4:\n","        img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n","    # img = img.squeeze(0)\n","    imgCropped = cropHand(img)  # cropHand expects RGB input\n","\n","    imgGray = cv2.cvtColor(imgCropped, cv2.COLOR_RGB2GRAY)\n","    imgGray = imgGray / 255.0\n","    imgGray = np.expand_dims(imgGray, axis=-1).astype(np.float32)\n","\n","    return imgGray, y\n","\n","\n","def tfPreprocess(x, y):\n","    img, label = tf.py_function(preprocessWithMediapipe, [x[0], y[0]], [tf.float32, tf.int32])\n","    img.set_shape([224, 224, 1])\n","    label.set_shape([])\n","    print(img.shape)\n","    return img, label\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","trainDataset = trainDataset.map(tfPreprocess, num_parallel_calls=AUTOTUNE).batch(32).prefetch(AUTOTUNE)\n","valDataset = valDataset.map(tfPreprocess, num_parallel_calls=AUTOTUNE).batch(32).prefetch(AUTOTUNE)\n"],"metadata":{"id":"smlbjC_ZBm1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for img, label in trainDataset.take(1):\n","    print(img.shape)  # should be (32,224,224,1)"],"metadata":{"id":"hRCov48sYCTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def createModel():\n","  model = keras.models.Sequential([\n","    layers.Input(shape=(224, 224, 1)),\n","\n","    layers.Conv2D(32, (3,3), activation='relu'),\n","    layers.MaxPooling2D((2,2)),\n","\n","    layers.Conv2D(64, (3,3), activation='relu'),\n","    layers.MaxPooling2D((2,2)),\n","\n","    layers.Conv2D(128, (3,3), activation='relu'),\n","    layers.MaxPooling2D((2,2)),\n","\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dense(29, activation='softmax')\n","  ])\n","\n","  return model\n","\n","# model = createModel()\n","model = keras.saving.load_model(\"/content/model.keras\")"],"metadata":{"id":"-IRE110YFswj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BatchCheckpoint(keras.callbacks.Callback):\n","    def __init__(self, save_path, save_every_n_batches=100):\n","        super().__init__()\n","        self.save_path = save_path\n","        self.save_every_n_batches = save_every_n_batches\n","        self.batch_count = 0\n","\n","    def on_batch_end(self, batch, logs=None):\n","        self.batch_count += 1\n","        if self.batch_count % self.save_every_n_batches == 0:\n","            self.model.save(f\"{self.save_path}_batch_{self.batch_count}.keras\")\n","            print(f\"Saved checkpoint at batch {self.batch_count}\")\n","\n","checkpoint_callback = BatchCheckpoint(save_path=\"model_checkpoint\", save_every_n_batches=100)"],"metadata":{"id":"TPWQXe7mkJQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(\n","    loss='sparse_categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"],"metadata":{"id":"i5eKyIRJJCUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    x = trainDataset,\n","    epochs = 100,\n","    validation_data = valDataset,\n","    callbacks=[checkpoint_callback]\n",")"],"metadata":{"id":"m0Yhf5_WKM6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save(\"model.keras\")"],"metadata":{"id":"KiSO7cCXJmu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CDRwTYgPOSLG"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}